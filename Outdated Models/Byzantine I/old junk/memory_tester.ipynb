{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 18:14:09.891641: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import random as rand\n",
    "\n",
    "from pympler.asizeof import asizeof\n",
    "\n",
    "def get_remaining_ram_in_gb():\n",
    "    # Get the available memory in bytes\n",
    "    available_memory = psutil.virtual_memory().available\n",
    "    # Convert bytes to GB\n",
    "    return available_memory / (1024 ** 3)\n",
    "\n",
    "from keras import Sequential, layers, regularizers, optimizers\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class relu3_Qagent_linearOut_dOut_l2():\n",
    "\n",
    "    class_hyperparameter_strings = '''alpha, epsilon_init, epsilon_decay, epsilon_min, gamma, layer1_size, \n",
    "                 layer2_size, layer3_size, layer4_size, batch_size, learning_rate,\n",
    "                 dropout1, dropout2, dropout3, reg1, reg2, reg3, memory, input_shape, actions'''.split(\", \")\n",
    "\n",
    "    def __init__(self, alpha, epsilon_init, epsilon_decay, epsilon_min, gamma, layer1_size, \n",
    "                 layer2_size, layer3_size, layer4_size, batch_size, learning_rate,\n",
    "                 dropout1, dropout2, dropout3, reg1, reg2, reg3, memory, input_shape, actions, sample_size_for_TDERR): #20 Hyperparameters!\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon_init \n",
    "        self.epsilon_decay = epsilon_decay \n",
    "        self.epsilon_min = epsilon_min \n",
    "        self.gamma = gamma \n",
    "        self.batch_size = batch_size\n",
    "        self.sample_size_for_TDERR = sample_size_for_TDERR\n",
    "        self.model = self.create_model(layer1_size=layer1_size, layer2_size=layer2_size, layer3_size=layer3_size, layer4_size=layer4_size, \n",
    "                                       dropout1=dropout1, dropout2=dropout2, dropout3=dropout3, reg1=reg1, reg2=reg2, reg3=reg3, learning_rate=learning_rate, input_shape=input_shape, output_size=actions)\n",
    "        \n",
    "        self.memory=deque(maxlen=memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model(layer1_size, layer2_size, layer3_size, layer4_size, \n",
    "                     dropout1, dropout2, dropout3, reg1, reg2, reg3, learning_rate, input_shape: tuple, output_size: int):\n",
    "        model = Sequential()\n",
    "        model.add(layers.Input(shape=input_shape))\n",
    "        model.add(layers.Dense(layer1_size, activation=\"relu\", kernel_regularizer=regularizers.l2(reg1)))\n",
    "        model.add(layers.Dropout(dropout1))\n",
    "        model.add(layers.Dense(layer2_size, activation=\"relu\", kernel_regularizer=regularizers.l2(reg2)))\n",
    "        model.add(layers.Dropout(dropout2))\n",
    "        model.add(layers.Dense(layer3_size, activation=\"relu\", kernel_regularizer=regularizers.l2(reg3)))\n",
    "        model.add(layers.Dropout(dropout3))\n",
    "        model.add(layers.Dense(layer4_size, activation=\"relu\"))\n",
    "        model.add(layers.Dense(output_size, activation=\"linear\"))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), \n",
    "                  loss='mean_squared_error',  # or another loss function depending on your task\n",
    "                  metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_weights(path)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save_weights(path)\n",
    "\n",
    "    def remember(self,   state, action, reward, nextstate,   done=False):\n",
    "        self.memory.append((state, action, reward, nextstate, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    def replay(self): #Adding beta to adjust for bias (self.model.optimiser.learning_rate = something with beta or whatever) might be a good idea\n",
    "        samples = rand.sample(self.memory, self.sample_size_for_TDERR)\n",
    "\n",
    "        guesses = [self.model.predict(experience[0].reshape(1, 1, 20)) for experience in samples]\n",
    "\n",
    "        newguesses = [self.model.predict(experience[3].reshape(1, 1, 20)) for experience in samples]\n",
    "\n",
    "        priorities = [abs(guess[0][0][experience[1]]-(experience[2] + np.max(newguess)*self.gamma )) for experience, guess, newguess in zip(samples, guesses, newguesses)]\n",
    "\n",
    "        \"\"\"Essentially:\n",
    "        priorities = []\n",
    "        for experience in self.memory:\n",
    "            state = experience[0]\n",
    "            action = experience[1]\n",
    "            reward = experience[2]\n",
    "            nextstate = experience[3]\n",
    "            #done = experience[4]\n",
    "\n",
    "            reward_from_action_guess = self.model.predict(state)[action]\n",
    "\n",
    "            actual_reward = reward + np.max(self.model.predict(nextstate))*self.gamma #bootstrapping guess\n",
    "\n",
    "            td_err = abs(reward_from_action_guess - actual_reward)\n",
    "\n",
    "            priorities.append(td_err)\"\"\"\n",
    "        \n",
    "        probabilities = np.array(priorities)**self.alpha\n",
    "        probabilities /= probabilities.sum()\n",
    "\n",
    "        indicies = np.random.choice(len(probabilities), size=self.batch_size, p=probabilities)\n",
    "\n",
    "        selected=[samples[i] for i in indicies]\n",
    "\n",
    "        states = np.array([experience[0].reshape(1, 20) for experience in selected]).reshape(self.batch_size,1,20)\n",
    "\n",
    "        actions = [experience[1] for experience in selected]\n",
    "\n",
    "        target_values = [(experience[2] + np.max(newguess)) if not experience[4] else experience[2] for experience, newguess in zip(selected, newguesses)]\n",
    "\n",
    "        \"\"\"Essentially:\n",
    "        target_values = []\n",
    "        for experience in selected:\n",
    "            if not experience[4]:\n",
    "                value = experience[2] + np.max(self.model.predict(experience[3]))\n",
    "            else:\n",
    "                value = experience[2]\n",
    "            target_values.append(value)\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = self.model.predict(states)\n",
    "\n",
    "        targets = np.array([[reward if i==action else pred for i, pred in enumerate(prediction)] for prediction, action, reward in zip(predictions, actions, target_values)])\n",
    "        \"\"\"\n",
    "        Generates a numpy array of target values by replacing specific predictions with corresponding rewards.\n",
    "        Essentially:\n",
    "        targets = []\n",
    "        for prediction, action, reward in zip(predictions, actions, target_values):\n",
    "            target = []\n",
    "            for i, pred in enumerate(prediction):\n",
    "                target.append(reward if i == action else pred)\n",
    "            targets.append(target)\n",
    "        targets = np.array(targets)\"\"\"\n",
    "\n",
    "        self.model.fit(states, targets, batch_size=self.batch_size, epochs=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5735015869140625\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "1/1 - 4s - 4s/step - loss: 1.1151 - mae: 0.1674\n",
      "2.492382049560547\n",
      "0.08111953735351562\n",
      "1351216\n"
     ]
    }
   ],
   "source": [
    "print(first:=get_remaining_ram_in_gb())\n",
    "\n",
    "agent = relu3_Qagent_linearOut_dOut_l2(alpha=0.8, epsilon_init=1, epsilon_decay=0.995, epsilon_min=0.1, gamma=0.995, layer1_size=1024, layer2_size=512, layer3_size=256, layer4_size=128, batch_size=10, learning_rate=0.0001, dropout1=0.5, dropout2=0.5, dropout3=0.5, reg1=0.001, reg2=0.001, reg3=0.001, memory=100000, input_shape=(1,20), actions=4, sample_size_for_TDERR=10)\n",
    "for i in range(100000):\n",
    "    agent.remember(np.array([[[1.0]*20]]), 2, 10.0, np.array([[[1.1]*20]]), False)\n",
    "agent.replay()\n",
    "\n",
    "#print(\"\")\n",
    "print(second:=get_remaining_ram_in_gb())\n",
    "#[print(\"\") for i in range(100)]\n",
    "print((first-second))\n",
    "#print(\"\")\n",
    "#print(agent.replay())#So garbage collector doesn't delete it \n",
    "#print(Sequential, layers, regularizers, optimizers, np, psutil, deque, rand, relu3_Qagent_linearOut_dOut_l2)\n",
    "\n",
    "print(asizeof(agent))\n",
    "print(asizeof(agent.memory))\n",
    "print(asizeof(agent.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351176\n",
      "825496\n",
      "524800\n"
     ]
    }
   ],
   "source": [
    "print(asizeof(agent))\n",
    "print(asizeof(agent.memory))\n",
    "print(asizeof(agent.model))#roughly 1351176 bytes for an agent (1.287 MB) so \n",
    "#create untill we have like 50 MB left for flex space and creation of on the fly junk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
